import os
import numpy as np
import tensorflow as tf
import utilities as utils
import utilities_image as utils_img
import random
from datetime import datetime

# To be changed to 2
from flip_training import*

NUM_CLASSES = 2

# To be changed to 400, 400
IMG_WIDTH = 400
IMG_HEIGHT = 400

# already run: 6
# Best 6 epochs
NUM_EPOCHS = 3

# To be changed to 3
NUM_CHANNELS = 3
# To be changed to 1
BATCH_SIZE = 2

TRAIN_SIZE = 1000*4
VALIDATION_SIZE = 50
TEST_SIZE = 100

RECORDING_STEP = 10

# initially set to 5e-4 but it is maybe too much
REGUL_PARAM = 1e-15

# Default is 0.001 for AdamOptimizer
LEARNING_RATE = 0.001

DROPOUT = 0.3

SEED = np.random.seed(237699)

STD_VAR_INIT = 0.1

BLOCK_NUMBER = 5

OUTPUT_PATH = 'conv_net_output/'
MODEL_PATH = OUTPUT_PATH + 'conv_net_model/conv_net_model.ckpt'
TRAINING_PATH = 'Skin_Data_set/flip_training/'
TEST_PATH = 'Skin_Data_set/test/'#test_set_good_format/'


PREDICTION_PATH = 'conv_net_prediction/'




def conv_net_model(x, keep_prob, phase_train):
    """
     data must be a 4D tensor generated by tf.constant()
      applied on a 4D numpy array with [image index, IMAGE_WIDTH, IMAGE_HEIGHT, CHANNEL]
    """

    # Could use tf.truncated_normal() instead of tf.random_normal(),
    # see what is the best choice
    # note tf.truncated() is used in the FCN implementation
    with tf.variable_scope('weights'):
        weights = {'W_d_conv1': utils.weight_def([3, 3, NUM_CHANNELS, 64], stddev=STD_VAR_INIT, name = 'W_d_conv1'),
                   'W_d_conv2': utils.weight_def([3, 3, 64, 64], stddev=STD_VAR_INIT, name = 'W_d_conv2'),
                   'W_d_conv3': utils.weight_def([3, 3, 64, 64], stddev=STD_VAR_INIT, name = 'W_d_conv3'),
                   'W_d_conv4': utils.weight_def([3, 3, 64, 64], stddev=STD_VAR_INIT, name = 'W_d_conv4'),
                   'W_d_conv5': utils.weight_def([3, 3, 64, 64], stddev=STD_VAR_INIT, name = 'W_d_conv5'),
                   'W_d_conv6': utils.weight_def([3, 3, 64, 64], stddev=STD_VAR_INIT, name = 'W_d_conv6'),
                   'W_d_conv7': utils.weight_def([3, 3, 64, 64], stddev=STD_VAR_INIT, name = 'W_d_conv7'),
                   'W_d_conv8': utils.weight_def([3, 3, 64, 64], stddev=STD_VAR_INIT, name = 'W_d_conv8'),
                   'W_d_conv9': utils.weight_def([3, 3, 64, 64], stddev=STD_VAR_INIT, name = 'W_d_conv9'),
                   'W_d_conv10': utils.weight_def([3, 3, 64, 64], stddev=STD_VAR_INIT, name = 'W_d_conv10'),
                   #'W_d_conv11': utils.weight_def([3, 3, 64, 64], stddev=STD_VAR_INIT, name = 'W_d_conv11'),
                   #'W_d_conv12': utils.weight_def([3, 3, 64, 64], stddev=STD_VAR_INIT, name = 'W_d_conv12'),

                   'W_trans1': utils.weight_def([3, 3, 64, 64], stddev=STD_VAR_INIT, name = 'W_trans1'),
                   'W_trans2': utils.weight_def([3, 3, 64, 64], stddev=STD_VAR_INIT, name = 'W_trans2'),

                   'W_deconv1': utils.weight_def([3, 3, 64, 64], stddev=STD_VAR_INIT, name = 'W_deconv1'),
                   'W_u_conv1': utils.weight_def([3, 3, 128, 96], stddev=STD_VAR_INIT, name = 'W_u_conv1'),
                   'W_u_conv2': utils.weight_def([3, 3, 96, 64], stddev=STD_VAR_INIT, name = 'W_u_conv2'),

                   'W_deconv2': utils.weight_def([3, 3, 64, 64], stddev=STD_VAR_INIT, name = 'W_deconv2'),
                   'W_u_conv3': utils.weight_def([3, 3, 128, 96], stddev=STD_VAR_INIT, name = 'W_u_conv3'),
                   'W_u_conv4': utils.weight_def([3, 3, 96, 64], stddev=STD_VAR_INIT, name = 'W_u_conv4'),

                   'W_deconv3': utils.weight_def([3, 3, 64, 64], stddev=STD_VAR_INIT, name = 'W_deconv3'),
                   'W_u_conv5': utils.weight_def([3, 3, 128, 96], stddev=STD_VAR_INIT, name = 'W_u_conv5'),
                   'W_u_conv6': utils.weight_def([3, 3, 96, 64], stddev=STD_VAR_INIT, name = 'W_u_conv6'),

                   'W_deconv4': utils.weight_def([3, 3, 64, 64], stddev=STD_VAR_INIT, name = 'W_deconv4'),
                   'W_u_conv7': utils.weight_def([3, 3, 128, 96], stddev=STD_VAR_INIT, name = 'W_u_conv7'),
                   'W_u_conv8': utils.weight_def([3, 3, 96, 64], stddev=STD_VAR_INIT, name = 'W_u_conv8'),

                   'W_deconv5': utils.weight_def([3, 3, 64, 64], stddev=STD_VAR_INIT, name = 'W_deconv5'),
                   'W_u_conv9': utils.weight_def([3, 3, 128, 96], stddev=STD_VAR_INIT, name = 'W_u_conv9'),
                   'W_u_conv10': utils.weight_def([3, 3, 96, 64], stddev=STD_VAR_INIT, name = 'W_u_conv10'),

                   #'W_deconv6': utils.weight_def([3, 3, 64, 64], stddev=STD_VAR_INIT, name = 'W_deconv6'),
                   #'W_u_conv11': utils.weight_def([3, 3, 128, 96], stddev=STD_VAR_INIT, name = 'W_u_conv11'),
                   #'W_u_conv12': utils.weight_def([3, 3, 96, 64], stddev=STD_VAR_INIT, name = 'W_u_conv12'),

                   'W_convout': utils.weight_def([1, 1, 64, NUM_CLASSES], stddev=STD_VAR_INIT, name = 'W_convout')}

    with tf.variable_scope('biases'):
        biases = {'B_d_conv1': utils.bias_def([64], name = 'B_d_conv1'),
                   'B_d_conv2': utils.bias_def([64], name = 'B_d_conv2'),
                   'B_d_conv3': utils.bias_def([64], name = 'B_d_conv3'),
                   'B_d_conv4': utils.bias_def([64], name = 'B_d_conv4'),
                   'B_d_conv5': utils.bias_def([64], name = 'B_d_conv5'),
                   'B_d_conv6': utils.bias_def([64], name = 'B_d_conv6'),
                   'B_d_conv7': utils.bias_def([64], name = 'B_d_conv7'),
                   'B_d_conv8': utils.bias_def([64], name = 'B_d_conv8'),
                   'B_d_conv9': utils.bias_def([64], name = 'B_d_conv9'),
                   'B_d_conv10': utils.bias_def([64], name = 'B_d_conv10'),
                   'B_d_conv11': utils.bias_def([64], name = 'B_d_conv11'),
                   'B_d_conv12': utils.bias_def([64], name = 'B_d_conv12'),

                   'B_trans1': utils.bias_def([64], name = 'B_trans1'),
                   'B_trans2': utils.bias_def([64], name = 'B_trans2'),

                   'B_deconv1': utils.bias_def([64], name = 'B_deconv1'),
                   'B_u_conv1': utils.bias_def([96], name = 'B_u_conv1'),
                   'B_u_conv2': utils.bias_def([64], name = 'B_u_conv2'),

                   'B_deconv2': utils.bias_def([64], name = 'B_deconv2'),
                   'B_u_conv3': utils.bias_def([96], name = 'B_u_conv3'),
                   'B_u_conv4': utils.bias_def([64], name = 'B_u_conv4'),

                   'B_deconv3': utils.bias_def([64], name = 'B_deconv3'),
                   'B_u_conv5': utils.bias_def([96], name = 'B_u_conv5'),
                   'B_u_conv6': utils.bias_def([64], name = 'B_u_conv6'),

                   'B_deconv4': utils.bias_def([64], name = 'B_deconv4'),
                   'B_u_conv7': utils.bias_def([96], name = 'B_u_conv7'),
                   'B_u_conv8': utils.bias_def([64], name = 'B_u_conv8'),

                   'B_deconv5': utils.bias_def([64], name = 'B_deconv5'),
                   'B_u_conv9': utils.bias_def([96], name = 'B_u_conv9'),
                   'B_u_conv10': utils.bias_def([64], name = 'B_u_conv10'),

                   'B_deconv6': utils.bias_def([64], name = 'B_deconv6'),
                   'B_u_conv11': utils.bias_def([96], name = 'B_u_conv11'),
                   'B_u_conv12': utils.bias_def([64], name = 'B_u_conv12'),

                   'B_convout': utils.bias_def([NUM_CLASSES], name = 'B_convout')}


    # Going down
    pool_layers = [x]
    conv_layers = []
    for i in range(BLOCK_NUMBER):
        if i==0:
            in_features = 3
        else:
            in_features = 64

        with tf.name_scope('pool' + str(i+1)):
            d_conv_dropout_relu1 = utils.conv2d_dropout_relu(pool_layers[i],\
                                                            weights['W_d_conv' + str(i*2+1)], biases['B_d_conv' + str(i*2+1)], in_features,\
                                                            keep_prob = keep_prob, phase_train = phase_train, name = 'd_conv_' + str(i*2+1))

            d_conv_dropout_relu2 = utils.conv2d_dropout_relu(d_conv_dropout_relu1,\
                                                            weights['W_d_conv' + str(i*2+2)], biases['B_d_conv' + str(i*2+2)], 64,\
                                                            keep_prob = keep_prob, phase_train = phase_train, name = 'd_conv_' + str(i*2+2))

            max_pool = utils.maxpool2d(d_conv_dropout_relu2, name = 'max_pool' + (str(i+1)))

        conv_layers += [d_conv_dropout_relu2]
        pool_layers += [max_pool]



    # Transition
    with tf.name_scope('transition'):
        d_conv_dropout_relu_trans1 = utils.conv2d_dropout_relu(pool_layers[-1], weights['W_trans1'], biases['B_trans1'], 64, keep_prob = keep_prob,\
                                                            phase_train = phase_train, name='trans1')

        d_conv_dropout_relu_trans2 = utils.conv2d_dropout_relu(d_conv_dropout_relu_trans1,\
                                                            weights['W_trans2'], biases['B_trans2'], 64,\
                                                            keep_prob = keep_prob, phase_train = phase_train, name ='trans2')

    # Going up
    deconv_layers = [d_conv_dropout_relu_trans2]
    for i in range(BLOCK_NUMBER):

        with tf.name_scope('deconv' + str(i+1)):
            deconv_relu1 = utils.deconv2d_relu(deconv_layers[i],\
                                                weights['W_deconv' + str(i+1)], biases['B_deconv' + str(i+1)], 64,\
                                                phase_train = phase_train, name = 'deconv' + str(i+1))

            concat1 = tf.concat([deconv_relu1, conv_layers[BLOCK_NUMBER - (i+1)]], axis = 3, name = 'concat' + str(i+1))

            u_conv_dropout_relu1 = utils.conv2d_dropout_relu(concat1,\
                                                weights['W_u_conv' + str(2*i+1)], biases['B_u_conv' + str(2*i+1)], 128,\
                                                keep_prob = keep_prob, phase_train = phase_train, name ='u_conv_' + str(2*i+1))

            u_conv_dropout_relu2 = utils.conv2d_dropout_relu(u_conv_dropout_relu1,\
                                                weights['W_u_conv' + str(2*i+2)], biases['B_u_conv' + str(2*i+2)], 96,\
                                                keep_prob = keep_prob, phase_train = phase_train, name ='u_conv_' + str(2*i+2))

            deconv_layers += [u_conv_dropout_relu2]


    # Convout
    with tf.name_scope('convout'):
        convout = utils.conv2d_dropout_relu(deconv_layers[-1], weights['W_convout'], biases['B_convout'], 64, keep_prob = tf.constant(1.0),\
                                                            phase_train = phase_train, name = 'convout')

    # Storing variables into a variables list
    # weights:
    variables = []
    for _, item in weights.items():
        variables.append(item)

    for _, item in biases.items():
        variables.append(item)


    return convout, variables




class ConvNet(object):

    def __init__(self):

        with tf.name_scope('input'):
            self.x = tf.placeholder(tf.float32, shape = [None, IMG_WIDTH, IMG_HEIGHT, NUM_CHANNELS])
            self.y = tf.placeholder(tf.float32, shape = [None, IMG_WIDTH, IMG_HEIGHT, NUM_CLASSES])
            self.keep_prob = tf.placeholder(tf.float32)
            self.phase_train = tf.placeholder(tf.bool)

        with tf.name_scope('logits'):
            logits, self.variables = conv_net_model(self.x, self.keep_prob, self.phase_train)

        with tf.name_scope('cost'):
            self.cost = self._get_cost(logits)

        with tf.name_scope('cross_entropy_sum'):
            self.cross_entropy_sum = tf.reduce_sum(\
                                        tf.nn.softmax_cross_entropy_with_logits(\
                                            logits=tf.reshape(logits, [-1, NUM_CLASSES]),
                                            labels=tf.reshape(self.y, [-1, NUM_CLASSES])))

        with tf.name_scope('cross_entropy'):
        # Only computed for the sake of summary in TensorBoard
            self.cross_entropy = tf.reduce_mean(utils.cross_entropy(tf.reshape(self.y, [-1, NUM_CLASSES]),
                                    tf.reshape(utils.pixel_wise_softmax_2(logits), [-1, NUM_CLASSES])))

        with tf.name_scope('softmax_predicter'):
            #self.predicter = utils.pixel_wise_softmax_2(logits)
            self.predicter = tf.nn.softmax(logits, dim = 3)

        with tf.name_scope('accuracy'):
            self.correct_pred = tf.equal(tf.argmax(self.predicter, 3), tf.argmax(self.y, 3))
            self.accuracy = tf.reduce_mean(tf.cast(self.correct_pred, tf.float32))

    def _get_cost(self, logits):
        flat_logits = tf.reshape(logits, [-1, NUM_CLASSES])
        flat_labels = tf.reshape(self.y, [-1, NUM_CLASSES])

        ########################### Loss function jaccard distance
        Num = tf.reduce_sum(flat_logits * flat_labels)
        De_num_1 = tf.reduce_sum(flat_logits * flat_logits)
        De_num_2 = tf.reduce_sum(flat_labels * flat_labels)
        loss = 1 - (Num / (De_num_1 + De_num_2 - Num))
        #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=flat_logits,
                                                                  #      labels=flat_labels))
        # add regularization
        with tf.name_scope('regularizers'):
            regularizers = sum([tf.nn.l2_loss(variable) for variable in self.variables])

        tf.summary.scalar('regularizers', REGUL_PARAM * regularizers)
        loss += REGUL_PARAM * regularizers

        return loss

    def predict(self, x_test):
        init = tf.global_variables_initializer()

        with tf.Session() as sess:
            sess.run(init)

            # Restoring the model from previous train
            # We take the model from the path specified in MODEL_PATH
            self.restore(sess, MODEL_PATH)

            y_useless = np.empty((BATCH_SIZE, IMG_WIDTH, IMG_HEIGHT, NUM_CLASSES))
            prediction = sess.run(self.predicter, feed_dict={self.x : x_test, self.y : y_useless, self.keep_prob : 1.,\
                                                                self.phase_train: False})

        return prediction

    def save(self, sess, model_save_path):
        saver = tf.train.Saver()
        save_path = saver.save(sess, model_save_path)

        return save_path

    def restore(self, sess, model_save_path):
        saver = tf.train.Saver()
        saver.restore(sess, model_save_path)



class Trainer(object):

    def __init__(self, conv_net):
        self.conv_net = conv_net

    def _get_optimizer(self, global_step):
        self.learning_rate_node = tf.Variable(LEARNING_RATE)
        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate_node)\
                                            .minimize(self.conv_net.cost, global_step=global_step)
        #optimizer = tf.train.AdadeltaOptimizer(learning_rate=self.learning_rate_node)\
        #                                    .minimize(self.conv_net.cost, global_step=global_step)
        return optimizer

    def _initialize(self):
        global_step = tf.Variable(0)

        # Definition of the summaries
        tf.summary.scalar('loss', self.conv_net.cost)
        tf.summary.scalar('cross_entropy', self.conv_net.cross_entropy)
        tf.summary.scalar('accuracy', self.conv_net.accuracy)

        with tf.name_scope('optimizer'):
            self.optimizer = self._get_optimizer(global_step)

        tf.summary.scalar('learning_rate', self.learning_rate_node)

        #summary_convout = get_image_summary(conv_net.logits)
        #tf.summary.image('convout', summary_convout)

        summary_predicter = utils_img.get_image_summary(self.conv_net.predicter)
        tf.summary.image('convout', summary_predicter)

        summary_argmax = utils_img.get_image_summary(tf.expand_dims(tf.argmax(self.conv_net.predicter, 3), 3))
        tf.summary.image('argmax_convout', summary_argmax)


        # Merging all summaries for tensor flow
        self.summary_op = tf.summary.merge_all()
        init = tf.global_variables_initializer()

        return init


    def train(self, restore=False):

        random.seed(datetime.now())


        if not os.path.exists(OUTPUT_PATH + 'validation_predictions/'):
            os.makedirs(OUTPUT_PATH + 'validation_predictions/')

        if not os.path.exists(OUTPUT_PATH + 'train_predictions/'):
            os.makedirs(OUTPUT_PATH + 'train_predictions/')

        print('-> Loading training set:')
        loaded_data = utils_img.load_images(TRAINING_PATH + "images/", TRAIN_SIZE)
        loaded_labels = utils_img.load_groundtruths(TRAINING_PATH + "groundtruth/", TRAIN_SIZE)

        train_indices = np.array(range(loaded_data.shape[0]))
        random.shuffle(train_indices)

        train_imgs = loaded_data[train_indices[VALIDATION_SIZE:]]
        train_labels = loaded_labels[train_indices[VALIDATION_SIZE:]]
        valid_imgs = loaded_data[train_indices[:VALIDATION_SIZE]]
        valid_labels = loaded_labels[train_indices[:VALIDATION_SIZE]]

        print("loaded data shape: ", loaded_data.shape)
        print("loaded labels shape: ", loaded_labels.shape)
        del loaded_labels
        del loaded_data

        print('Number of training images loaded:', train_imgs.shape[0])
        print('Number of training groundtruths loaded:', train_labels.shape[0])

        print('Number of validation images loaded:', '0')#valid_imgs.shape[0])
        print('Number of validation groundtruths loaded:', '0')#valid_labels.shape[0])




        init = self._initialize()

        with tf.Session() as sess:
            # writing the graph
            tf.train.write_graph(sess.graph_def, OUTPUT_PATH + 'summary/', 'graph.pb', False)

            sess.run(init)

            #print('trainable:', tf.trainable_variables(scope=None))

            if restore:
                ckpt = tf.train.get_checkpoint_state('./submitted_conv_net_model/')
                if ckpt and ckpt.model_checkpoint_path:
                    print('Restoring previous train...')
                    self.conv_net.restore(sess, ckpt.model_checkpoint_path)

            summary_writer = tf.summary.FileWriter(OUTPUT_PATH + 'summary/', graph=sess.graph)

            hm_epochs = NUM_EPOCHS
            image_indices = np.array(range(train_imgs.shape[0]))
            for epoch in range(hm_epochs):
                print()
                print('-> Epoch', epoch+1, 'starting...')
                random.shuffle(image_indices)
                epoch_loss = 0
                for step in range(int((TRAIN_SIZE - VALIDATION_SIZE)/(BATCH_SIZE*4*2))):
                    # feeding epoch_x and epoch_y for training current batch (to replace with our own )
                    epoch_x = train_imgs[[image_indices[step * BATCH_SIZE : step * BATCH_SIZE + BATCH_SIZE]]]
                    epoch_y = train_labels[[image_indices[step * BATCH_SIZE : step * BATCH_SIZE + BATCH_SIZE]]]

                    if step % RECORDING_STEP == 0:
                        summary_str, _, loss, lr, predictions = sess.run((self.summary_op, self.optimizer, self.conv_net.cost, self.learning_rate_node, self.conv_net.predicter),\
                                                            feed_dict={self.conv_net.x: epoch_x, self.conv_net.y: epoch_y,
                                                                        self.conv_net.keep_prob: DROPOUT, self.conv_net.phase_train: True})

                        glob_step = epoch *int((TRAIN_SIZE - VALIDATION_SIZE)/BATCH_SIZE) + step
                        utils_img.compare_proba_pred(predictions[[0]], epoch_x[0],\
                                                    OUTPUT_PATH + 'train_predictions/step' + str(glob_step) + '.png')
                        summary_writer.add_summary(summary_str, glob_step)
                        summary_writer.flush()
                    else:
                        # sees.run() evaluate a tensor
                        # the first argument of sess.run() is an array corresponding to every operation needed
                        # Feeding sess.run() with y is necessary because cost_op needs it
                        _, loss, lr = sess.run((self.optimizer, self.conv_net.cost, self.learning_rate_node),\
                                                            feed_dict={self.conv_net.x: epoch_x, self.conv_net.y: epoch_y,
                                                                        self.conv_net.keep_prob: DROPOUT, self.conv_net.phase_train: True})
                    epoch_loss += loss
                    print('Step', step+1, 'completed out of', int((TRAIN_SIZE - VALIDATION_SIZE)/BATCH_SIZE), '/ step loss:', loss,\
                            ' / batch:', image_indices[step * BATCH_SIZE : step * BATCH_SIZE + BATCH_SIZE])

                print('epoch_x shape:', epoch_x.shape)
                print('epoch_y shape:', epoch_y.shape)

                print()
                print('===> Epoch', epoch+1, 'completed out of', hm_epochs, '/ epoch train loss:', epoch_loss)
                print('batch size: ' + str(BATCH_SIZE) +')')

                print('Saving epoch ' + str(epoch+1) + ':')
                model_path = self.conv_net.save(sess, MODEL_PATH)

                # In the future convert it in test set
                valid_loss_arr = np.zeros(VALIDATION_SIZE)
                valid_accuracy_arr = np.zeros(VALIDATION_SIZE)
                for i in range(0, VALIDATION_SIZE):
                    img = valid_imgs[[i]]
                    groundtruth = valid_labels[[i]]
                    valid_loss, valid_accuracy = self.output_stats(sess, img, groundtruth)
                    valid_loss_arr[i] = valid_loss
                    valid_accuracy_arr[i] = valid_accuracy

                    if i % (RECORDING_STEP * 2) == 0:
                        self.store_prediction(sess, img, groundtruth,\
                                    save_path = OUTPUT_PATH + "validation_predictions/image_{a}_epoch_{b}.png".format(a=(i+1), b=epoch))
                print('Validation stats / epoch:', epoch + 1)
                print('Averaged validation loss:', np.mean(valid_loss_arr))
                print('Averaged validation accuracy:', np.mean(valid_accuracy_arr))
            return model_path


    def output_stats(self, sess, batch_x, batch_y):
        # Calculate batch loss and accuracy
        loss, acc,pred = sess.run([self.conv_net.cost, self.conv_net.accuracy, self.conv_net.predicter],
                                feed_dict={self.conv_net.x: batch_x, self.conv_net.y: batch_y, self.conv_net.keep_prob: 1.,\
                                            self.conv_net.phase_train: False})

        return loss, acc*100



    def store_prediction(self, sess, test_x, test_y, save_path):
        predictions = sess.run(self.conv_net.predicter, feed_dict={self.conv_net.x: test_x, self.conv_net.y: test_y, self.conv_net.keep_prob: 1.,\
                                                                    self.conv_net.phase_train: False})

        utils_img.compare_proba_pred(predictions, np.squeeze(test_x, axis = 0), save_path)




def execute(restore_flag = True, prediction_flag=True):
    conv_net = ConvNet()
    trainer = Trainer(conv_net)
    save_model_path = trainer.train(restore = restore_flag)
    print('Model saved in:', save_model_path)

    if prediction_flag == True:
        if not os.path.exists(OUTPUT_PATH + '400_400_test_pred/'):
            os.makedirs(OUTPUT_PATH + '400_400_test_pred/')

        if not os.path.exists(OUTPUT_PATH + 'test_pred/'):
            os.makedirs(OUTPUT_PATH + 'test_pred/')

        if not os.path.exists(OUTPUT_PATH + 'submission/'):
            os.makedirs(OUTPUT_PATH + 'submission/')

       # data_test = utils_img.load_test_images(TEST_PATH, TEST_SIZE)
        data_test = flip_training.load_images_test(TEST_PATH + "images_test/", TEST_SIZE)
        data_gt_test= flip_training.load_groundtruths_test(TEST_PATH + "GroundTruth_test/", TEST_SIZE)

        print (data_test.shape)
        print (data_test[0].shape)

        merged_prediction_array = []
        #for i in range(int(TEST_SIZE/4)):
        #pred_array_400 = []
        ''' for j in range(4):
        test_predictions = conv_net.predict(data_test[[i*4+j]])
        #print(i*4+j)
        utils_img.compare_proba_pred(test_predictions, data_test[i*4+j],\
                                     OUTPUT_PATH + '400_400_test_pred/prediction_test_' + str(i) +'_'+ str(j) + '.png')
            pred_array_400 += [np.squeeze(test_predictions, 0)]
        #print('i',i)
        merged_prediction = utils_img.merge_400_400_pred(pred_array_400)
        utils_img.save_image(merged_prediction, OUTPUT_PATH + 'test_pred/prediction_test_' + str(i+1) + '.png')
        merged_prediction_array += [merged_prediction]
        print('test_' + str(i+1) + ' computed')'''

        #utils.write_submission(merged_prediction_array, OUTPUT_PATH + 'submission/submission25.csv', threshold=0.25)
        acc =  []
        for i in range(int(TEST_SIZE)):
            pred_array_400=[]
            test_predictions = conv_net.predict(data_test[[i]])

            print('test',test_predictions)
            #print(test_predictions[[0]].shape)
            print('data', data_test[[i]].shape)
            utils_img.compare_proba_pred(test_predictions[[0]], data_test[[i]], \
                                        OUTPUT_PATH + '400_400_test_pred/prediction_test_' + str(i) + '.png')
            #print (test_predictions.shape)
            #print(test_predictions[0])
            merged_img = np.zeros((400, 400))
            merged_img += test_predictions[0][:400, :400 , 0]


            pred_array_400 += [np.squeeze(test_predictions, 0)]
            #for k in range (merged_img.shape[0]):
             #   for j in range (merged_img.shape[1]):
              #      if merged_img[k][j] > 0.05:
               #         merged_img[k][j] = 1
            # print('i',i)
#            merged_prediction = utils_img.merge_400_400_pred(pred_array_400)
            utils_img.save_image(merged_img, OUTPUT_PATH + 'test_pred/prediction_test_' + str(i + 1) + '.png')

            merged_prediction_array += [pred_array_400]
            print('test_' + str(i + 1) + ' computed')
            error=0
            union=0
            inter=0
            for k in range (merged_img.shape[0]):
                for j in range (merged_img.shape[1]):
                    union += (merged_img[k][j] > 0.2 or data_gt_test[i][k][j] > 0.5)
                    inter += (merged_img[k][j] > 0.2 and data_gt_test[i][k][j] > 0.5)
            error = abs(inter / union)
            acc.append(1-error)
            print ('Error of the test :', acc[i])
            del merged_img
        print('Mean error for the all test set :', np.mean(acc))